{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Loan_status_2007-2020Q3.csv\", nrows = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# save the head of the dataset to a csv file\n",
    "df.head().to_csv('head.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 : Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "    'Unnamed: 0',  # Index or row number\n",
    "    'id',          # Unique identifier\n",
    "    'grade',       # Risk category\n",
    "    'sub_grade',   # Sub-category of risk\n",
    "    'int_rate',    # Interest rate\n",
    "    #'last_pymnt_amnt', \n",
    "    #'last_fico_range_high', 'last_fico_range_low',\n",
    "    'funded_amnt', 'funded_amnt_inv',  # Funding amounts\n",
    "    'total_pymnt', 'total_rec_prncp', 'total_rec_int',  # Total payments\n",
    "    'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date',  \n",
    "    'hardship_length', 'hardship_dpd', 'hardship_loan_status',  \n",
    "    'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount', \n",
    "    'debt_settlement_flag' # Debt settlement indicator\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_cleaned = df_cleaned[df_cleaned['loan_status'].isin(['Fully Paid', 'Charged Off'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last_pymtn_amount, last_fico_range_high, last_fico_range_low have a strong influence on the loan status.but if we assume that we are within the loan repayment period for prediction after the loan is approved , they can be used to predict the loan status. as they are the most 3 important features in the dataset, they can be used to predict the loan status. keepin them will give us good models as we will say later in the notebook.\n",
    "This works under the assumptiom that we are within the loan repayment period for prediction after the loan is approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all column names\n",
    "print(df_cleaned.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional columns to drop\n",
    "additional_columns_to_drop = [\n",
    "    'url', 'issue_d', \n",
    "    'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d',\n",
    "    'out_prncp', 'out_prncp_inv', 'total_pymnt_inv', 'total_rec_late_fee',\n",
    "    'recoveries', 'collection_recovery_fee',\n",
    "    'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'revol_bal_joint',\n",
    "    'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_earliest_cr_line',\n",
    "    'total_rev_hi_lim', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit',\n",
    "    'total_il_high_credit_limit',\n",
    "    'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status',\n",
    "    'deferral_term', 'hardship_amount',\n",
    "    'policy_code' # contains only 1 and nan\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "df_cleaned = df_cleaned.drop(columns=additional_columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for missing values (e.g., more than 50% missing)\n",
    "threshold = 0.5\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = df_cleaned.isna().mean()\n",
    "\n",
    "# Create a list of columns with missing values greater than the threshold\n",
    "columns_with_too_many_missing_values = missing_percentage[missing_percentage > threshold].index.tolist()\n",
    "\n",
    "df_cleaned = df_cleaned.drop(columns=columns_with_too_many_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate the data based on loan_status\n",
    "fully_paid = df_cleaned[df_cleaned['loan_status'] == 'Fully Paid']\n",
    "charged_off = df_cleaned[df_cleaned['loan_status'] == 'Charged Off']\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot histogram for Fully Paid loans\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(fully_paid['loan_amnt'], bins=50, kde=True, color='blue', stat='density')\n",
    "plt.title('Distribution of Loan Amount (Fully Paid)')\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Plot histogram for Charged Off loans\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(charged_off['loan_amnt'], bins=50, kde=True, color='red', stat='density')\n",
    "plt.title('Distribution of Loan Amount (Charged Off)')\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='loan_status', y='loan_amnt', data=df_cleaned)\n",
    "plt.title('Loan Amount by Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Loan Amount')\n",
    "# Rotate x-axis labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each category\n",
    "loan_status_counts = df_cleaned['loan_status'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(loan_status_counts.index, loan_status_counts.values, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(ticks=[0, 1], labels=['Fully Paid', 'Charged Off'])  # Assuming 0 is Fully Paid and 1 is Charged Off\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data we will undersample the majority class to balance the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map loan status to binary values\n",
    "df_cleaned['loan_status'] = df_cleaned['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1})\n",
    "\n",
    "# Verify the change\n",
    "print(df_cleaned['loan_status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = object_columns.drop(['zip_code', 'revol_util', 'earliest_cr_line', 'initial_list_status',  'emp_title', 'home_ownership',\n",
    "       'verification_status', 'title', 'addr_state','application_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_cleaned, columns=columns_to_encode, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Possible Modeling Approaches\n",
    "\n",
    "### 1. Logistic Regression\n",
    "\n",
    "**Example:** Implement a `LogisticRegression` model to predict `loan_status`. Logistic Regression is a simple yet effective model for binary classification problems.\n",
    "\n",
    "**Pros:**\n",
    "- **Interpretability:** Coefficients indicate how features influence the outcome, making it easy to understand.\n",
    "- **Simplicity:** Quick to train and requires minimal computational resources.\n",
    "- **Baseline Model:** Serves as a strong baseline for comparison against more complex models.\n",
    "\n",
    "**Cons:**\n",
    "- **Linearity Assumption:** Assumes a linear relationship between features and the log-odds of the target, which may not always be accurate.\n",
    "- **Limited Complexity:** May struggle to capture non-linear relationships and feature interactions.\n",
    "\n",
    "### 2. Random Forest\n",
    "\n",
    "**Example:** Use a `RandomForestClassifier` to handle complex interactions between features and improve prediction accuracy.\n",
    "\n",
    "**Pros:**\n",
    "- **Performance:** Generally offers high accuracy due to its ensemble nature, which reduces overfitting.\n",
    "- **Feature Importance:** Provides insights into which features are most important in making predictions.\n",
    "- **Robustness:** Handles both numerical and categorical data well, and is less sensitive to outliers.\n",
    "\n",
    "**Cons:**\n",
    "- **Interpretability:** More challenging to interpret than Logistic Regression, as it involves multiple decision trees.\n",
    "- **Computational Cost:** Requires more computational power and memory, especially with large datasets or many trees.\n",
    "\n",
    "if the data is linearly separable, Logistic Regression is generally a good choice. However, if the relationship is more complex or non-linear, Random Forest may be a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Remove object variables\n",
    "df_numeric = df_encoded.drop(columns=df_encoded.select_dtypes(include=['object']).columns)\n",
    "# Drop rows with any NaN values\n",
    "df_numeric = df_numeric.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imbalance\n",
    "\n",
    "Before training our classifier, we will address the data imbalance by using undersampling. This technique involves randomly reducing the number of instances in the \"Fully Paid\" class to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "X = df_numeric.drop(columns='loan_status')  # Features\n",
    "y = df_numeric['loan_status']  # Target\n",
    "\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply undersampling to the entire dataset\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# Display the new class distribution\n",
    "print(f'Original class distribution:\\n{y.value_counts()}')\n",
    "print(f'Resampled class distribution:\\n{y_resampled.value_counts()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the resampled dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(f'Training set size: {len(y_train)}')\n",
    "print(f'Test set size: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation with target variable (loan_status)\n",
    "correlation = df_numeric.corr()['loan_status'].sort_values(ascending=False)\n",
    "\n",
    "# plot the correlation the top 10 most positive and negative correlated features\n",
    "plt.figure(figsize=(10, 12))\n",
    "correlation.drop('loan_status').plot(kind='barh')\n",
    "plt.title('Correlation with Loan Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling\n",
    "We scale X_test using the parameters used to scale X_train in order to avoid data bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Step 4: Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 : Model implementation and explanability\n",
    "\n",
    "For the question 3, we will implement a logistic regression model and explain the coefficients of the model. as well as a random forest model with shap values to explain the model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line (random chance)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can conclude that the model perfoms well with an accuracy with more than 0.92 setting a baseline for the random forest model. And also that the features are lienarly related to the target variable.\n",
    "\n",
    "`note` : dropping the features last_pymnt_amnt, last_fico_range_high, last_fico_range_low will decrease the accuracy of the model to approximately 0.68 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 18))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Feature Importance in Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 : Model explainability LR\n",
    "\n",
    "## Interpretation and Feature Importance: \n",
    "\n",
    "The magnitude of the coefficient indicates the strength of the relationship. Larger coefficients (in absolute value) indicate that changes in the feature have a larger impact on the predicted probability.\n",
    "\n",
    "The sign (positive or negative) of a coefficient indicates the direction of the relationship between the feature and the target. A positive coefficient means that as the feature value increases, the likelihood of the positive class (e.g., Fully Paid) increases. Conversely, a negative coefficient means that an increase in the feature value decreases the likelihood of the positive class.\n",
    "\n",
    "Here the most important features are `last_pymnt_amnt`, `last_fico_range_high`, `last_fico_range_low`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`note` : dropping the features last_pymnt_amnt, last_fico_range_high, last_fico_range_low will decrease the accuracy of the model to approximately 0.66 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# # Save the model using joblib\n",
    "# \n",
    "# model_filename = 'random_forest_model.joblib'\n",
    "# joblib.dump(rf_model, model_filename)\n",
    "# print(f'Model saved to {model_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to hold the feature names and their importance\n",
    "import pandas as pd\n",
    "\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(12, 18))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importances in Random Forest Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 : Model explainability RF\n",
    "\n",
    "While the magnitude of the coefficient determines the strength of the effect of the feature on the target, we can't know in wich direction the feature will affect the target.\n",
    "\n",
    "For this case we will use shap values to interpret the effect of the features on the target  \n",
    "**warning** : shap values are computationally expensive and may take a long time to compute here we will use a small sample of the data to compute the shap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "X_test_scaled_subset = X_test_scaled[:50]\n",
    "\n",
    "explainer = shap.Explainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary plot shows the feature importance of each feature in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHAP summary plot provides a comprehensive view of how each feature influences the predictions of your model for a given class. above for class 0\n",
    "\n",
    "#### Understanding the Plot :\n",
    "\n",
    "- **Features on the Y-Axis:** Each row represents a feature from the dataset. Features are sorted by their importance in influencing the model's predictions.\n",
    "\n",
    "- **SHAP Values on the X-Axis:** The horizontal axis shows the SHAP values, which indicate the impact of each feature on the model's prediction. Positive SHAP values push the prediction higher, while negative values pull it lower.\n",
    "\n",
    "- **Color Coding:** The color of each point represents the value of the feature. Blue for low values and red for high values is used.\n",
    "\n",
    "- **Distribution of Points:** Each point represents a single prediction instance. The spread of points along the x-axis indicates how much variation there is in the feature's impact across different predictions. A wider spread suggests more variability in feature impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 : Additional Steps for Model Enhancement\n",
    "\n",
    "If I had more time or resources, I would take the following steps to enhance the model, improve its performance, and increase robustness:\n",
    "\n",
    "### 1. Hyperparameter Tuning\n",
    "To optimize the performance of the `RandomForestClassifier`, I would perform more thorough hyperparameter tuning using the following methods:\n",
    "- **Grid Search or Random Search:** To find the optimal set of parameters by exhaustively searching through specified parameter values.\n",
    "- **Bayesian Optimization:** To efficiently explore the hyperparameter space, which might be more effective than grid search.\n",
    "\n",
    "### 2. Model Ensembling\n",
    "I would create an ensemble of different models to capture various patterns in the data:\n",
    "- **Combine Multiple Models:** Build an ensemble using models like Gradient Boosting Machines and XGBoost.\n",
    "- **Stacking Models:** Implement a stacking approach where predictions from multiple models are combined using a meta-model.\n",
    "\n",
    "### 3. Advanced Feature Selection\n",
    "To improve the model's efficiency and interpretability, I would employ advanced feature selection techniques:\n",
    "- **Recursive Feature Elimination (RFE):** Systematically select the most important features by recursively eliminating less important ones.\n",
    "- **L1 Regularization (Lasso):** Use Lasso regression for feature selection by penalizing less important features.\n",
    "\n",
    "### 4. Cross-Validation\n",
    "To ensure model performance is consistent , I would:\n",
    "- **K-Fold Cross-Validation:** Implement K-Fold cross-validation to assess model performance across multiple subsets.\n",
    "\n",
    "### 5. Explainability and Interpretability\n",
    "To enhance model transparency and stakeholder trust:\n",
    "- **More Explainability Techniques:** Explore other explainability methods such as LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "### 6. Robustness Checks\n",
    "To ensure the robustness of the model:\n",
    "- **Outlier Detection:** Perform outlier detection and removal to improve the model's performance.\n",
    "- **Adversarial Validation:** Ensure the training and test data are from the same distribution, avoiding data leakage.\n",
    "\n",
    "### 7. Model Deployment\n",
    "To ensure the model is production-ready:\n",
    "- **Deploy on Cloud:** Deploy the model as a microservice using cloud platforms like AWS, Google Cloud, or Azure.\n",
    "- **Monitoring and Maintenance:** Implement monitoring for model performance in production, enabling detection and mitigation of model drift or data shift over time.\n",
    "\n",
    "### Additional Datasets to Enrich the Original Dataset\n",
    "\n",
    "To improve the model's predictive accuracy and provide more context, I would consider integrating the following datasets:\n",
    "\n",
    "\n",
    "### 1. Economic Indicators\n",
    "- **Macroeconomic Data:** Include macroeconomic indicators like unemployment rates, inflation, interest rates, and GDP growth from sources such as the Federal Reserve or World Bank.\n",
    "\n",
    "### 2. Behavioral Data\n",
    "- **Transaction Data:** If available, integrate transaction-level data from borrowers' bank accounts or credit cards to gain insights into spending patterns.\n",
    "\n",
    "### 3. Web Scraping\n",
    "- **Additional Sources:** I’d also consider scraping the web for additional relevant data, for example adding more years of data to the dataset.\n",
    "I have checked and the Lending Club does not allow web scraping after 2020.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 : Scalability Discussion\n",
    "\n",
    "### a. Number of Loans/Rows in the Training Data\n",
    "\n",
    "The scalability of my solution largely depends on the amount of data it needs to process during training. As the number of loans/rows in the dataset increases, several factors come into play:\n",
    "\n",
    "1. **Memory Usage:** \n",
    "   - The `RandomForestClassifier` is an ensemble learning method that constructs multiple decision trees during training. The memory required increases with the size of the training data and the number of trees. To use fully the provided dataset, we need more high memory, potentially requiring more RAM or distributed computing resources.\n",
    "\n",
    "2. **Training Time:**\n",
    "   - Training time increases with the number of rows in the dataset. While `RandomForestClassifier` is relatively efficient, large datasets can still result in lengthy training times. To address this, I could:\n",
    "     - **Use Parallel Processing:** Leverage multi-core CPUs to train trees in parallel, which can significantly reduce training time.\n",
    "     - **Reduce Data Dimensionality:** Employ feature selection or dimensionality reduction techniques to minimize the number of features, thereby speeding up the training process.\n",
    "     - **Use Cloud Resources:** Utilize cloud platforms with scalable resources (e.g., AWS EC2 instances with large memory or distributed computing like Spark) to handle larger datasets efficiently.\n",
    "\n",
    "\n",
    "### b. Number of Predictions in Production\n",
    "\n",
    "The scalability of the inference endpoint in production depends on the number of predictions it needs to make and the speed at which these predictions are required:\n",
    "\n",
    "1. **Inference Latency:**\n",
    "   - **Low Latency Requirements:** If the application requires real-time or near-real-time predictions, I need to ensure the model can make predictions quickly. `RandomForestClassifier` is generally fast at inference due to its tree-based structure, but latency could become an issue with an extremely large number of requests. To mitigate this:\n",
    "     - **Optimized Deployment:** Deploy the model on a powerful server or use specialized hardware (e.g., GPUs or TPUs) to speed up inference.\n",
    "     - **Model Simplification:** Consider simplifying the model by reducing the number of trees or using a different algorithm that offers faster inference times.\n",
    "\n",
    "2. **Batch vs. Real-Time Predictions:**\n",
    "   - **Batch Processing:** For scenarios where predictions can be made in bulk (e.g., processing thousands of loan applications overnight), I could batch the requests and process them in parallel, optimizing resource usage.\n",
    "   - **Asynchronous Processing:** Implement asynchronous processing where predictions are queued and processed as resources become available, which is useful in managing bursts of prediction requests.\n",
    "\n",
    "3. **Containerization and Microservices:**\n",
    "   - **Docker:** Packaging the model in a Docker container ensures that it can be easily scaled across different environments. By deploying the container on a Kubernetes cluster, I can dynamically scale the number of replicas based on the load, ensuring high availability and scalability.\n",
    "   - **Microservices Architecture:** Decompose the prediction service into microservices that can independently scale, allowing different components (e.g., feature preprocessing, model inference, post-processing) to scale according to demand.\n",
    "\n",
    "4. **Monitoring and Auto-Scaling:**\n",
    "   - Implement monitoring tools to track the performance of the inference endpoint in production. Using tools like Prometheus and Grafana, I can set up auto-scaling rules that trigger the deployment of additional resources when the load increases, ensuring the system remains responsive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's use pca to visualize the data in a 2D or 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uns = scaler.transform(X_resampled)\n",
    "X_uns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# For PCA (2D)\n",
    "pca = PCA(n_components=3)\n",
    "X_reduced_pca = pca.fit_transform(X_uns)\n",
    "\n",
    "# print the variance ratio of the PCA\n",
    "print(f'Explained variance ratio: {pca.explained_variance_ratio_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert PCA results and labels to a DataFrame\n",
    "df_pca = pd.DataFrame(X_reduced_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "y_aligned = y_resampled.reset_index(drop=True)\n",
    "df_pca['Loan Status'] = y_aligned\n",
    "\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_pca, x='PC1', y='PC2', z='PC3',\n",
    "    color='Loan Status',\n",
    "    title='3D Scatter Plot of PCA (Principal Component Analysis)',\n",
    "    labels={'Loan Status': 'Loan Status'}\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_traces(marker=dict(size=4), selector=dict(mode='markers'))\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the figure, we can see that the classes are well separated in the 3D space, indicating that the features are highly representative of the classes.We can hope that the clustering algorithms will perform well on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_uns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the cluster unique values\n",
    "print(f'Cluster labels: {set(clusters)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using kmeans to cluster the data into 2 clusters and visualize the clusters in a 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca['Cluster'] = clusters  # Add clusters to the DataFrame\n",
    "\n",
    "# Create a combined feature for Loan Status and Cluster\n",
    "df_pca['LoanCluster'] = df_pca['Loan Status'].astype(str) + '_' + df_pca['Cluster'].astype(str)\n",
    "\n",
    "# Define a color map for each unique combination of Loan Status and Cluster\n",
    "color_map = {\n",
    "    '0_0': 'blue',  # Loan Status 0, Cluster 0\n",
    "    '0_1': 'green',  # Loan Status 0, Cluster 1\n",
    "    '1_0': 'red',   # Loan Status 1, Cluster 0\n",
    "    '1_1': 'purple'  # Loan Status 1, Cluster 1\n",
    "}\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_pca, x='PC1', y='PC2', z='PC3',\n",
    "    color='LoanCluster',\n",
    "    title='KMeans Clustering with Loan Status',\n",
    "    color_discrete_map=color_map,\n",
    "    labels={'color': 'LoanCluster'}\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_traces(marker=dict(size=5), selector=dict(mode='markers'))\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster, calculate the percentage of Fully Paid and Charged Off loans\n",
    "cluster_status = df_pca.groupby(['Cluster', 'Loan Status']).size().unstack()\n",
    "cluster_status['Total'] = cluster_status.sum(axis=1)\n",
    "cluster_status['Fully Paid (%)'] = cluster_status[0] / cluster_status['Total']\n",
    "cluster_status['Charged Off (%)'] = cluster_status[1] / cluster_status['Total']\n",
    "cluster_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans clustering here doesn't provide meaningful separation, with both clusters showing nearly identical distributions of fully paid and charged off loans. This suggests the data forms a single, indistinct cloud, making KMeans being a simple algorithm insufficient. If i have more ressources hierarchical clustering could be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Community Detection\n",
    "\n",
    "If the dataset can be represented as a graph (e.g., users connected by similar loan characteristics), we can apply community detection algorithms to identify groups of similar nodes and see how the model's predictions vary across these communities. The issue is to find an edge relationship between the nodes in the graph that have a meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
